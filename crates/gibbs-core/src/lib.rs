use serde::{Deserialize, Serialize};
use std::collections::HashMap;

pub mod parse;

/// References a specific model.
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum Model {
    Gemma27B3,
    Qwen235B3,
    Phi4,
    Gemini2Flash,
}

impl Model {
    fn all() -> &'static [Model] {
        use Model::*;
        &[Gemma27B3, Qwen235B3, Phi4, Gemini2Flash]
    }

    pub fn openrouter_str(&self) -> &'static str {
        use Model::*;
        match self {
            Gemma27B3 => &"google/gemma-3-27b-it",
            Qwen235B3 => &"qwen/qwen3-235b-a22b",
            Phi4 => &"microsoft/phi-4",
            Gemini2Flash => &"google/gemini-2.0-flash-001",
        }
    }

    pub fn make_prompt<S: Into<String>>(&self, prompt: S) -> ChatMessage {
        use Model::*;
        match self {
            Phi4 => ChatMessage::user(prompt),
            _ => ChatMessage::system(prompt),
        }
    }
}

impl<'a> TryFrom<&'a str> for Model {
    type Error = ();

    fn try_from(s: &'a str) -> Result<Model, Self::Error> {
        for candidate in Model::all().into_iter() {
            if candidate.openrouter_str() == s {
                return Ok(candidate.clone());
            }
        }
        Err(())
    }
}

pub mod backends;

pub trait CompletionBackend: Send {
    /// Returns information about the model this backend is wired to.
    fn get_model(&self) -> &Model;

    /// Implements one model call to complete a turn in an LLM conversation. The workhorse of this trait.
    fn call(
        &self,
        messages: Vec<ChatMessage>,
    ) -> impl std::future::Future<Output = Result<CompletionsResponse, Box<dyn std::error::Error>>> + Send;

    /// Easy method to prompt a model and get the response as a string.
    fn simple_call<S: Into<String> + Send>(
        &self,
        prompt: S,
    ) -> impl std::future::Future<Output = Result<String, Box<dyn std::error::Error>>> {
        async {
            let res = self
                .call(vec![self.get_model().make_prompt(prompt)])
                .await?;
            match res.choices.into_iter().next().unwrap().message.content {
                Some(c) => Ok(c),
                None => Err("unexpected: no message content".into()),
            }
        }
    }
}

/// The serialized format representing the output of a turn in an LLM conversation.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ChatMessage {
    /// Role: system, user, assistant, tool
    pub role: String,

    /// Content of the message, optional when using tool calls
    #[serde(skip_serializing_if = "Option::is_none")]
    pub content: Option<String>,
}

impl ChatMessage {
    pub fn user<S: Into<String>>(s: S) -> Self {
        ChatMessage {
            role: "user".into(),
            content: Some(s.into()),
        }
    }
    pub fn system<S: Into<String>>(s: S) -> Self {
        ChatMessage {
            role: "system".into(),
            content: Some(s.into()),
        }
    }
}

/// The serialized description of one response from the model.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ChatChoice {
    /// Index of this choice in the array of choices
    pub index: usize,

    /// The message generated by the model
    pub message: ChatMessage,

    /// Reason why the model stopped generating
    pub finish_reason: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub(crate) struct OpenrouterProvider {
    #[serde(skip_serializing_if = "Vec::is_empty")]
    pub ignore: Vec<String>,
}

/// A request to the OpenAI Chat Completions API.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub(crate) struct CompletionsRequest {
    /// Model identifier to use for completion
    pub model: String,

    pub provider: OpenrouterProvider,

    /// Model input and output
    pub messages: Vec<ChatMessage>,
}

impl Default for CompletionsRequest {
    fn default() -> Self {
        Self {
            model: Model::Gemma27B3.openrouter_str().into(),
            messages: vec![],
            provider: OpenrouterProvider {
                ignore: vec!["Nebius".into(), "Kluster".into()],
            },
        }
    }
}

/// A response from the OpenAI Completions API.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CompletionsResponse {
    /// Unique ID
    #[serde(default)]
    pub id: String,

    /// Object type, always "chat.completion"
    pub object: Option<String>,

    #[serde(default)]
    pub created: u64,
    #[serde(default)]
    pub model: String,

    /// Array of completion choices
    #[serde(default)]
    pub choices: Vec<ChatChoice>,
}

/// Returns the WAN IP from which this code is accessing the internet.
pub async fn wan_ip() -> Result<String, Box<dyn std::error::Error>> {
    let resp = reqwest::get("https://httpbin.org/ip")
        .await?
        .json::<HashMap<String, String>>()
        .await?;

    resp.get("origin")
        .cloned()
        .ok_or("Missing 'origin' key".into())
}

/// Makes a very simple, text-in-text-out model call.
pub async fn model_call<S: Into<String> + Send>(
    model: Model,
    prompt: S,
) -> Result<String, Box<dyn std::error::Error>> {
    backends::OpenrouterModel {
        model,
        api_key: None,
    }
    .simple_call(prompt)
    .await
}

#[cfg(test)]
mod tests {
    // use super::*;

    #[test]
    fn smoke() {}
}
